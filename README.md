# ğŸ“š E-Book Summarizer (Lightweight LLM Version)

A local, privacy-friendly tool to summarize books and research PDFs using the lightweight `gemma:2b` model via [Ollama](https://ollama.com). Everything runs in your browser via a simple Streamlit UI â€” **no cloud APIs or GPU required**.

---

## âœ¨ Features

- âœ… Accepts EPUB and PDF files
- ğŸ” Automatically chunks chapters (~2000 tokens each)
- ğŸ“ Summarizes each chunk with `gemma:2b`
- ğŸ§  Title generation for each section
- ğŸ§¾ Exports summaries as CSV and Markdown
- âš™ï¸ Runs entirely offline using Ollama and CPU

---

## ğŸ›  Setup

### 1. Clone the repository & create a virtual environment

```bash
git clone https://github.com/your-username/ollama-ebook-summary
cd ollama-ebook-summary
python -m venv venv
venv\Scripts\activate  # for Windows
# OR
source venv/bin/activate  # for Mac/Linux
```

---

### 2. Install required Python packages

```bash
pip install -r requirements.txt
```

---

### 3. Install Ollama & pull the small LLM

- Download and install Ollama: https://ollama.com/download
- Pull the Gemma 2B model:

```bash
ollama pull gemma:2b
```

---


---

## ğŸš€ How to Run

```bash
streamlit run streamlit_app.py
```

Once running, open in your browser at:  
ğŸ“ [http://localhost:8501](http://localhost:8501)

---

## ğŸ“‚ Output

After processing, your output will be saved in:

- `out/{book-id}/...` â€“ split PDFs by chapter
- `out/{book-id}.csv` â€“ raw section metadata
- `{book-id}_gemma:2b.csv` â€“ final summary results
- `{book-id}_gemma:2b.md` â€“ formatted markdown summary

---

## ğŸ§  About Chunking

Each chapter/section is chunked into ~2000 token blocks based on:
- Table of Contents (for EPUBs and tagged PDFs)
- Manual fallback if ToC metadata is missing

Why 2000 tokens?  
ğŸ‘‰ Research shows LLM reasoning stabilizes around 2000-3000 tokens ([source](https://huggingface.co/papers/2402.14848)).

---

## ğŸ“Œ Notes

- You only need to run `streamlit_app.py` â€” no manual CLI steps!
- `gemma:2b` is small and runs on CPU â€” perfect for laptops or low-end PCs.
- You can later upgrade to heavier models if you want better quality.

---

## âœ… Requirements

- Python 3.11+
- Ollama installed and running in the background
- At least 4 GB free RAM recommended

---

## ğŸ§  Credits & Acknowledgements

- Model: [Gemma 2B](https://ollama.com/library/gemma) via Ollama
- Inspired by open-source research summarization workflows


